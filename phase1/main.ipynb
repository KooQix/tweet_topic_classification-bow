{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1 - Evaluate annotators and get true label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoded_annotations(df: pd.DataFrame, num_annotators: int):\n",
    "\t\"\"\"Encode the labels\n",
    "\n",
    "\tArgs:\n",
    "\t\tdf (pd.DataFrame): The original data from the csv file\n",
    "\t\tnum_annotators (int): The number of annotators\n",
    "\n",
    "\tReturns:\n",
    "\t\ttuple(2): (\n",
    "\t\t\tHoneHotEncoder: The encoder object used to encode the labels\n",
    "\t\t\tlist: The encoded annotations for each annotator\n",
    "\t\t)\n",
    "\t\"\"\"\n",
    "\tcols = df.columns\n",
    "\n",
    "\t# Fill the missing labels with the new label \"missing\"\n",
    "\tdf = df.fillna(\"missing\")\n",
    "\n",
    "\t# Encode each label using One-Hot-Encoding technique\n",
    "\tencoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "\t# Concatenate each annotations from all annotators annotators to encode the labels\n",
    "\tres = encoder.fit_transform(np.array([\n",
    "\t\tdf[cols[1 + i]] for i in range(num_annotators)\n",
    "\t\t]).reshape(-1, 1))\n",
    "\n",
    "\t# Re affect the annotations to each annotator, encoded\n",
    "\tchunk_size = len(res) // num_annotators\n",
    "\n",
    "\tres_annot = []\n",
    "\n",
    "\tfor i in range(num_annotators - 1):\n",
    "\t\tres_annot.append(res[i * chunk_size : (i+1) * chunk_size])\n",
    "\n",
    "\tres_annot.append(res[(num_annotators - 1) * chunk_size :])\n",
    "\t\t\n",
    "\tprint(f\"Categories: {list(encoder.categories_[0])}\")\n",
    "\n",
    "\treturn (encoder, res_annot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kappa score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kappa(annot_1, annot_2):\n",
    "\t\"\"\"Calculate the kappa, between annotator 1 and 2\n",
    "\n",
    "\tArgs:\n",
    "\t\tannot_1 (list): Encoded annotations of annotator 1\n",
    "\t\tannot_2 (list): Encoded annotations of annotator 2\n",
    "\n",
    "\tReturns:\n",
    "\t\tfloat: The kappa score between the two annotators\n",
    "\t\"\"\"\n",
    "\tassert len(annot_1) == len(annot_2)\n",
    "\n",
    "\treturn cohen_kappa_score(annot_1.flatten(), annot_2.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(df: pd.DataFrame):\n",
    "\t\"\"\"Get the average kappa score for each annotator for a given file\n",
    "\n",
    "\tArgs:\n",
    "\t\tdf (pd.DataFrame): The data\n",
    "\n",
    "\tReturns:\n",
    "\t\ttuple(3): (\n",
    "\t\t\tOneHotEncoder: The encoder object used to encode the labels\n",
    "\t\t\tlist: The encoded annotations\n",
    "\t\t\tlist: The average kappa score for each annotator (list[0] == average kappa for annotator 1)\n",
    "\t\t)\n",
    "\t\"\"\"\n",
    "\n",
    "\tnum_annotators = len(df.columns) - 1\n",
    "\n",
    "\t# Encode the labels\n",
    "\tencoder, annotators = get_encoded_annotations(df, num_annotators)\n",
    "\n",
    "\t# Calculate the average kappa for each annotator\n",
    "\tscores_annot = []\n",
    "\tfor i in range(num_annotators):\n",
    "\t\tkappa = 0\n",
    "\t\tfor j in range(num_annotators):\n",
    "\t\t\tif i == j: continue\n",
    "\t\t\tkappa += get_kappa(annotators[i], annotators[j])\n",
    "\n",
    "\t\tscores_annot.append(kappa / (num_annotators - 1))\n",
    "\n",
    "\n",
    "\treturn (encoder, annotators, scores_annot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categories(encoder: OneHotEncoder):\n",
    "\t\"\"\"Get the categories encoded by the encoder\n",
    "\n",
    "\tArgs:\n",
    "\t\tencoder (OneHotEncoder): The encoder used to encode the labels\n",
    "\n",
    "\tReturns:\n",
    "\t\tlist: The decoded labels\n",
    "\t\"\"\"\n",
    "\treturn list(encoder.categories_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_encoding(encoder: OneHotEncoder, input_encoded: np.array):\n",
    "\t\"\"\"Reverse the encoding One-hot-encoding => string\n",
    "\n",
    "\tArgs:\n",
    "\t\tencoder (OneHotEncoder): The encoder used to encode the labels\n",
    "\t\tinput_encoded (np.array): The encoded label\n",
    "\n",
    "\tReturns:\n",
    "\t\tstr: The decoded label\n",
    "\t\"\"\"\n",
    "\tcategories = get_categories(encoder)\n",
    "\tassert input_encoded.shape == (len(categories),)\n",
    "\n",
    "\treturn categories[list(input_encoded).index(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_encoding_list(encoder: OneHotEncoder, input_encoded: np.array):\n",
    "\t\"\"\"Reverse the encoding for a list of encoded labels\n",
    "\n",
    "\tArgs:\n",
    "\t\tencoder (OneHotEncoder): The encoder used to encode the labels\n",
    "\t\tinput_encoded (np.array): The list of encoded labels\n",
    "\n",
    "\tReturns:\n",
    "\t\tlist: The lits of decoded labels\n",
    "\t\"\"\"\n",
    "\tcategories = get_categories(encoder)\n",
    "\n",
    "\tres = []\n",
    "\n",
    "\tfor i in range(len(input_encoded)):\n",
    "\t\tres.append(reverse_encoding(encoder, input_encoded[i]))\n",
    "\n",
    "\treturn res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keys_from_value(dict: dict, value: any):\n",
    "\t\"\"\"Return the keys associated to a value in a dictionary\n",
    "\n",
    "\tArgs:\n",
    "\t\tdict (dict): The dictionary to get the keys from\n",
    "\t\tvalue (any): The value to look for\n",
    "\n",
    "\tReturns:\n",
    "\t\tlist: The list of keys where (key, value)\n",
    "\t\"\"\"\n",
    "\tkeys = list(dict.keys())\n",
    "\tvalues = list(dict.values())\n",
    "\n",
    "\treturn [\n",
    "\t\tkeys[i] for i in range(len(keys)) \n",
    "\t\tif values[i] == value\n",
    "\t\t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(file: str):\n",
    "\t\"\"\"Associate text => true label for a given file\n",
    "\n",
    "\tArgs:\n",
    "\t\tfile (str): The file to process\n",
    "\n",
    "\tReturns:\n",
    "\t\tpd.DataFrame: The dataframe containing text => true label\n",
    "\t\"\"\"\n",
    "\t# Load data from file\n",
    "\tdf = pd.read_csv(file, header=None).drop(0)\n",
    "\n",
    "\tencoder, encoded_annotations, kappa_values = get_scores(df)\n",
    "\n",
    "\tunreliable_annot = [i for i in range(len(kappa_values)) if kappa_values[i] < 0.2]\n",
    "\n",
    "\tprint(f\"Unreliable annotators (starts at 0): {unreliable_annot}\")\n",
    "\n",
    "\t# Remove kappa score for unreliable annotators\n",
    "\tkappa_values = [\n",
    "\t\tkappa_values[i] for i in range(len(kappa_values)) \n",
    "\t\tif i not in unreliable_annot\n",
    "\t]\n",
    "\n",
    "\t# Get annotations of reliable annotators\n",
    "\tannotations = [\n",
    "\t\treverse_encoding_list(encoder, encoded_annotations[i]) \n",
    "\t\tfor i in range(len(encoded_annotations)) \n",
    "\t\tif i not in unreliable_annot\n",
    "\t\t]\n",
    "\n",
    "\n",
    "\t# Init dataframe with the text and empty label\n",
    "\tfinal_df = pd.DataFrame()\n",
    "\tfinal_df = final_df.reindex(columns = [\"text\", \"label\"])   \n",
    "\tfinal_df[\"text\"] = df[df.columns[0]]      \n",
    "\tfinal_df[\"label\"] = final_df[\"label\"].astype(str)\n",
    "\n",
    "\t# Fill in the label for each text\n",
    "\tfor row in range(len(annotations[0])):\n",
    "\n",
    "\t\t# Init each label to 0 occurrence\n",
    "\t\tres = {\n",
    "\t\t\tkey: 0 for key in get_categories(encoder)\n",
    "\t\t}\n",
    "\n",
    "\t\t# Count the number of occurrence of each label for a given sample\n",
    "\t\tfor annotator in range(len(annotations)):\n",
    "\t\t\tres[annotations[annotator][row]] += 1\n",
    "\n",
    "\t\t# Get the majority labels\n",
    "\t\tmax_val = max(res.values())\n",
    "\t\tkeys = get_keys_from_value(res, max_val)\n",
    "\n",
    "\t\t# A distinct majority \n",
    "\t\tif (len(keys) == 1): final_df.at[row + 1, \"label\"] = keys[0]\n",
    "\n",
    "\t\t# Unclear => assign the label (one with the majority votes) of the annotator with the max kappa\n",
    "\t\telse:\n",
    "\t\t\t# Max first\n",
    "\t\t\tordered_kappa = kappa_values.copy()\n",
    "\t\t\tordered_kappa.sort(reverse=True)\n",
    "\n",
    "\t\t\ti = 0\n",
    "\t\t\twhile i < len(ordered_kappa):\n",
    "\t\t\t\tindex_annot = kappa_values.index(ordered_kappa[i])\n",
    "\t\t\t\tlabel_annot_max_kappa = annotations[index_annot][row]\n",
    "\n",
    "\t\t\t\t# If the annotator with the current max kappa annotated such as its annotation is in the majority, assign this label, else next annotator, based on decreasing kappa score\n",
    "\t\t\t\tif label_annot_max_kappa in keys:  \n",
    "\t\t\t\t\tfinal_df.at[row + 1, \"label\"] = label_annot_max_kappa\n",
    "\t\t\t\t\tbreak\n",
    "\n",
    "\t\t\t\ti += 1\n",
    "\n",
    "\treturn final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verif_no_na(df: pd.DataFrame):\n",
    "\t\"\"\"Check if a text has no assigned label\n",
    "\n",
    "\tArgs:\n",
    "\t\tdf (pd.DataFrame): The dataframe to look at\n",
    "\t\"\"\"\n",
    "\tindexes_na = []\n",
    "\n",
    "\t# I noticed df.isna().value.any() did not detect \"nan\", so I implemented it\n",
    "\tfor i in range(df.shape[0]):\n",
    "\t\tif df[df.columns[1]][i+1] == \"nan\":\n",
    "\t\t\tindexes_na.append(i+1)\n",
    "\n",
    "\t# Return the indexes of the missing labels, if any\n",
    "\tif len(indexes_na) > 0:\n",
    "\t\tprint(\"Na values detected at indexes (start at 1):\")\n",
    "\t\tprint(indexes_na)\n",
    "\t\tprint(\"\\n\")\n",
    "\telse:\n",
    "\t\tprint(\"All good, no na value\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_and_write(list: list, output_file: str):\n",
    "\t\"\"\"Concatenate the list of dataframe and write the result in output_file\n",
    "\n",
    "\tArgs:\n",
    "\t\tlist (list): The list of dataframes to concatenate\n",
    "\t\toutput_file (str): The name of the output file\n",
    "\t\"\"\"\n",
    "\tdf = pd.concat([df for df in list], ignore_index=True)\n",
    "\tdf.to_csv(output_file)\n",
    "\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### changeorg_stance\n",
    "\n",
    "\n",
    "_I modified one file, containing 2 labels for a given annotator / sample, since these documents contain mutually exclusive labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories: ['anti-mitigation', 'missing', 'pro-mitigation', 'unclear']\n",
      "Unreliable annotators (starts at 0): [1]\n",
      "All good, no na value\n",
      "\n",
      "Categories: ['anti-mitigation', 'missing', 'pro-mitigation', 'unclear']\n",
      "Unreliable annotators (starts at 0): [3]\n",
      "All good, no na value\n",
      "\n",
      "Categories: ['anti-mitigation', 'missing', 'pro-mitigation', 'unclear']\n",
      "Unreliable annotators (starts at 0): []\n",
      "All good, no na value\n",
      "\n",
      "Categories: ['anti-mitigation', 'missing', 'pro-mitigation', 'unclear']\n",
      "Unreliable annotators (starts at 0): []\n",
      "All good, no na value\n",
      "\n",
      "Categories: ['anti-mitigation', 'missing', 'pro-mitigation', 'unclear']\n",
      "Unreliable annotators (starts at 0): []\n",
      "All good, no na value\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df0_changeorg = process(\"./Datasets/changeorg_stance/changeorg_stance_0.csv\")\n",
    "verif_no_na(df0_changeorg)\n",
    "\n",
    "df1_changeorg = process(\"./Datasets/changeorg_stance/changeorg_stance_1.csv\")\n",
    "verif_no_na(df1_changeorg)\n",
    "\n",
    "df2_changeorg = process(\"./Datasets/changeorg_stance/changeorg_stance_2.csv\")\n",
    "verif_no_na(df2_changeorg)\n",
    "\n",
    "df3_changeorg = process(\"./Datasets/changeorg_stance/changeorg_stance_3.csv\")\n",
    "verif_no_na(df3_changeorg)\n",
    "\n",
    "df4_changeorg = process(\"./Datasets/changeorg_stance/changeorg_stance_4.csv\")\n",
    "verif_no_na(df4_changeorg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_and_write([df0_changeorg, df1_changeorg, df2_changeorg, df3_changeorg, df4_changeorg], \"changeorg_stance.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nyt_stance\n",
    "\n",
    "I modified 2 files (_1 and _2), containing 2 labels for a given annotator / sample, since these documents contain mutually exclusive labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories: ['anti-mitigation', 'missing', 'pro-mitigation', 'unclear']\n",
      "Unreliable annotators (starts at 0): [2, 3]\n",
      "All good, no na value\n",
      "\n",
      "Categories: ['anti-mitigation', 'pro-mitigation', 'unclear']\n",
      "Unreliable annotators (starts at 0): []\n",
      "All good, no na value\n",
      "\n",
      "Categories: ['anti-mitigation', 'missing', 'pro-mitigation', 'unclear']\n",
      "Unreliable annotators (starts at 0): [1]\n",
      "All good, no na value\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df0_nyt = process(\"./Datasets/nyt_stance/nyt_stance_0.csv\")\n",
    "verif_no_na(df0_nyt)\n",
    "\n",
    "df1_nyt = process(\"./Datasets/nyt_stance/nyt_stance_1.csv\")\n",
    "verif_no_na(df1_nyt)\n",
    "\n",
    "df2_nyt = process(\"./Datasets/nyt_stance/nyt_stance_2.csv\")\n",
    "verif_no_na(df2_nyt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_and_write([df0_nyt, df1_nyt, df2_nyt], \"nyt_stance.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('.venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a1d81e0ab65a2d871dd04cd5480301015f0912bc0455ad9e82832e105402504a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
